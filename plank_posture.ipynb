{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path, ms=100):\n",
    "    \n",
    "    # Video Capture 객체 생성\n",
    "    capture = cv2.VideoCapture(path)\n",
    "    \n",
    "    # Frame을 저장할 List 선언\n",
    "    frames = []\n",
    "    \n",
    "    while capture.isOpened(): # Video Capture가 준비되었는지 확인\n",
    "        \n",
    "        run, frame = capture.read() # 다음 Frame 읽기\n",
    "        # ROI 설정 (x, y, width, height)\n",
    "        roi_x, roi_y, roi_w, roi_h = 0, 280, 852, 200\n",
    "        \n",
    "        # ROI 추출\n",
    "        roi = frame[roi_y:roi_y + roi_h, roi_x:roi_x + roi_w]\n",
    "\n",
    "        # 그레이스케일 변환\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "        edges = cv2.Canny(blurred, 50, 150)\n",
    "        # 원본 프레임에 엣지 결과를 덮어쓰기 (ROI 영역만)\n",
    "        frame[roi_y:roi_y + roi_h, roi_x:roi_x + roi_w] = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        \n",
    "        if run: # Frame을 읽은 경우\n",
    "            # 결과 출력\n",
    "            cv2.imshow('Frame with ROI Edges', frame)\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else: # 재생이 완료되어 더 이상 Frame을 읽을 수 없는 경우\n",
    "            break\n",
    "        \n",
    "        # Frame List에 추가\n",
    "        frames.append(frame)\n",
    "\n",
    "    capture.release() # Capture 자원 반납\n",
    "    cv2.destroyAllWindows() # 창 제거\n",
    "    \n",
    "    return np.array(frames, dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video가 저장된 경로 입력\n",
    "PATH = r\".\\plank_sample2.mp4\"\n",
    "# PATH = r\"C:\\Users\\admin\\Desktop\\KUDIP\\Video Samples\\Hand Video2.mov\"\n",
    "# PATH = r\"C:\\Users\\hj\\AICV\\수업\\디지털영상처리\\Video Samples\\highway.mov\"\n",
    "# PATH = r\"C:\\Users\\hj\\AICV\\수업\\디지털영상처리\\Video Samples\\earth.avi\"\n",
    "\n",
    "# Video 재생 및 반환 (Numpy Array)\n",
    "video = load_video(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Data Type: <class 'numpy.ndarray'>\n",
      "- Data Shape: (261, 480, 852, 3) *Frames x Height x Width x Channel\n",
      "- Maximum Intensity: 255\n",
      "- Minimum Intensity: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"- Data Type: {type(video)}\")\n",
    "print(f\"- Data Shape: {video.shape} *Frames x Height x Width x Channel\")\n",
    "print(f\"- Maximum Intensity: {video.max()}\")\n",
    "print(f\"- Minimum Intensity: {video.min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def high_boost_filtering_bgr(image, kernel_size, alpha):\n",
    "    # 이미지 각 채널에 대해 고역통과 필터링 적용\n",
    "    b, g, r = cv2.split(image)\n",
    "    \n",
    "    # 각 채널에 대해 고역통과 필터링 수행\n",
    "    b_filtered = high_boost_filtering_channel(b, kernel_size, alpha)\n",
    "    g_filtered = high_boost_filtering_channel(g, kernel_size, alpha)\n",
    "    r_filtered = high_boost_filtering_channel(r, kernel_size, alpha)\n",
    "\n",
    "    # 고역통과 필터링된 이미지 병합\n",
    "    result = cv2.merge((b_filtered, g_filtered, r_filtered))\n",
    "\n",
    "    return result\n",
    "\n",
    "def high_boost_filtering_channel(channel, kernel_size, alpha):\n",
    "    # 라플라시안 필터 생성\n",
    "    laplacian_kernel = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])\n",
    "\n",
    "    # 채널에 라플라시안 필터 적용\n",
    "    laplacian = cv2.filter2D(channel, -1, laplacian_kernel)\n",
    "\n",
    "    # 고역통과 필터링된 이미지 생성\n",
    "    high_boost = channel + alpha * laplacian\n",
    "\n",
    "    # 이미지 범위를 0~255로 조정\n",
    "    high_boost = np.clip(high_boost, 0, 255)\n",
    "    high_boost = high_boost.astype(np.uint8)\n",
    "\n",
    "    return high_boost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 영상 읽기\n",
    "cap = cv2.VideoCapture('plank_sample2.mp4')\n",
    "\n",
    "def get_centroid(contour):\n",
    "    M = cv2.moments(contour)\n",
    "    if M[\"m00\"] != 0:\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "        return (cX, cY)\n",
    "    else:\n",
    "        return (0, 0)\n",
    "    \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    # ROI 설정 (x, y, width, height)\n",
    "    roi_x, roi_y, roi_w, roi_h = 0, 280, 852, 200\n",
    "    \n",
    "    # ROI 추출\n",
    "    roi = frame[roi_y:roi_y + roi_h, roi_x:roi_x + roi_w]\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 그레이스케일 변환 및 가우시안 블러 적용\n",
    "    \n",
    "    kernel_size = 3\n",
    "    alpha = 1.5\n",
    "    result = high_boost_filtering_bgr(roi, kernel_size, alpha)\n",
    "    gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "    median_blurred_image = cv2.medianBlur(gray, 3)\n",
    "\n",
    "    # Laplacian 필터를 적용하여 에지를 검출합니다.\n",
    "    log_edges = cv2.Laplacian(median_blurred_image, cv2.CV_64F)\n",
    "\n",
    "    # 결과 이미지를 8비트로 변환합니다.\n",
    "    log_edges = np.uint8(np.absolute(log_edges))\n",
    "    \n",
    "    cv2.imshow('log_edges', log_edges)\n",
    "\n",
    "    # # 객체 추출\n",
    "    # contours, _ = cv2.findContours(log_edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # # 상체와 하체의 중심점 초기화\n",
    "    # upper_body_contours = []\n",
    "    # lower_body_contours = []\n",
    "\n",
    "    # # 프레임의 중앙을 기준으로 상체와 하체를 나눔\n",
    "    # roi_height = roi.shape[0]\n",
    "    # roi_center_y = roi_height // 2\n",
    "\n",
    "    # # 컨투어 처리\n",
    "    # for contour in contours:\n",
    "    #     area = cv2.contourArea(contour)\n",
    "    #     if area > 1000:  # 노이즈 제거를 위한 최소 면적 설정\n",
    "    #         x, y, w, h = cv2.boundingRect(contour)\n",
    "    #         centroid = get_centroid(contour)\n",
    "    #         if centroid[1] < roi_center_y:  # 상체로 간주\n",
    "    #             upper_body_contours.append(contour)\n",
    "    #         else:  # 하체로 간주\n",
    "    #             lower_body_contours.append(contour)\n",
    "\n",
    "    # # 상체와 하체의 컨투어 그리기\n",
    "    # cv2.drawContours(roi, upper_body_contours, -1, (0, 255, 0), 2)  # 초록색: 상체\n",
    "    # cv2.drawContours(roi, lower_body_contours, -1, (255, 0, 0), 2)  # 파란색: 하체\n",
    "\n",
    "    # 결과 출력\n",
    "    # cv2.imshow('Canny', edges)\n",
    "    # cv2.imshow('Plank Pose - Upper and Lower Body', roi)\n",
    "    # cv2.imshow('otsu', t_otsu)\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def background_subtraction(frame, fgbg):\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    return fgmask\n",
    "\n",
    "def detect_largest_contour(fgmask):\n",
    "    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        return largest_contour\n",
    "    return None\n",
    "\n",
    "def draw_contour(frame, contour, color=(0, 255, 0)):\n",
    "    cv2.drawContours(frame, [contour], -1, color, 2)\n",
    "\n",
    "def get_contour_center(contour):\n",
    "    M = cv2.moments(contour)\n",
    "    if M[\"m00\"] != 0:\n",
    "        cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "        return (cx, cy)\n",
    "    return None\n",
    "\n",
    "# 비디오 캡처 초기화\n",
    "cap = cv2.VideoCapture('plank_sample2.mp4')\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# 기준선 설정 (예제에서는 화면 중앙)\n",
    "ret, frame = cap.read()\n",
    "frame_height, frame_width = frame.shape[:2]\n",
    "baseline = 100\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    roi_x, roi_y, roi_w, roi_h = 0, 280, 852, 200\n",
    "    \n",
    "    # ROI 추출\n",
    "    frame = frame[roi_y:roi_y + roi_h, roi_x:roi_x + roi_w]\n",
    "    \n",
    "    fgmask = background_subtraction(frame, fgbg)\n",
    "    largest_contour = detect_largest_contour(fgmask)\n",
    "\n",
    "    if largest_contour is not None:\n",
    "        contour_center = get_contour_center(largest_contour)\n",
    "        \n",
    "        if contour_center:\n",
    "            cx, cy = contour_center\n",
    "            # 엉덩이 위치를 기준으로 위 또는 아래로 벗어났는지 확인\n",
    "            if cy < baseline - 30:\n",
    "                color = (0, 0, 255)  # 빨간색: 너무 올라감\n",
    "                message = \"Too High\"\n",
    "            elif cy > baseline + 30:\n",
    "                color = (255, 0, 0)  # 파란색: 너무 내려감\n",
    "                message = \"Too Low\"\n",
    "            else:\n",
    "                color = (0, 255, 0)  # 초록색: 올바른 위치\n",
    "                message = \"Correct\"\n",
    "                \n",
    "            draw_contour(frame, largest_contour, color)\n",
    "            cv2.circle(frame, (cx, cy), 5, color, -1)\n",
    "            cv2.putText(frame, message, (cx, cy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    # 기준선을 그립니다.\n",
    "    cv2.line(frame, (0, baseline), (frame_width, baseline), (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.imshow('Plank Pose Detection', frame)\n",
    "    \n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
